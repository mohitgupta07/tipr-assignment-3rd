{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn.datasets\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import scipy\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "import datetime\n",
    "from skimage import io\n",
    "np.random.seed(42)\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "import itertools\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argv = list(sys.argv)\n",
    "dataSetName = argv[argv.index('--dataset')+1]\n",
    "test_path = argv[argv.index('--test-data')+1]\n",
    "if '--train-data' in argv:\n",
    "    train_path = argv[argv.index('--train-data')+1]\n",
    "    config   = []#HiddenLayers for now.\n",
    "    model_path=\"\"\n",
    "    for st in argv[argv.index('--configuration')+1:]:\n",
    "        st  = st.strip()\n",
    "        if st.endswith(']'):a\n",
    "            config.append(int(st.strip('[]')))\n",
    "            break\n",
    "        else:\n",
    "            config.append(int(st.strip('[]')))\n",
    "    if(dataSetName==\"MNIST\"):\n",
    "        X_train,y_train,X_val,y_val,classes=MNIST()\n",
    "        net,X_val,y_val=getNET(X_train,y_train,X_val,y_val,dataSetName,[512, 128, 128],['relu', 'swish', 'swish'],classes)\n",
    "    elif(dataSetName==\"Cat-Dog\" or dataSetName==\"cat-dog\" ):\n",
    "        X_train,y_train,X_val,y_val,classes=catdog(28)\n",
    "        net,X_val,y_val=getNET(X_train,y_train,X_val,y_val,dataSetName,[512, 128, 128],['relu', 'swish', 'swish'],classes)\n",
    "        \n",
    "    net.train(initADAMS=False,batch_size=1000,doOp=False,epochs=50,KKK=1,earlyStopping=True,X_val=X_val,y_val=y_val,printResults=True,minEpochs=1,patience=0)  \n",
    "    net.saveModel(\"tempModel_{0}\".format(dataSetName))\n",
    "    model_path=\"tempModel_{0}.npy\".format(dataSetName)\n",
    "    \n",
    "else:\n",
    "    if(dataSetName==\"MNIST\"):\n",
    "           model_path=\"MNIST_model/Model_Main.py\"\n",
    "\n",
    "    elif(dataSetName==\"Cat-Dog\" or dataSetName==\"cat-dog\" ):\n",
    "           model_path=\"cat-dog_model/Model_Main.py\"\n",
    "\n",
    "if(dataSetName==\"MNIST\"):\n",
    "    TestMNIST(test_path,model_path)\n",
    "elif(dataSetName==\"Cat-Dog\" or dataSetName==\"cat-dog\" ):\n",
    "     Testcatdog(test_path,model_path) \n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X,Y,size=0.2):\n",
    "        #Cross-validation -- to be done via k-fold later.\n",
    "        from sklearn.model_selection import train_test_split  \n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=size)\n",
    "        return X_train, Y_train,X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scale(X,normalize=False,gaxis=1):\n",
    "#     from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "#     scaler = StandardScaler()\n",
    "#     if(normalize):\n",
    "#         X= sklearn.preprocessing.normalize(X,axis=gaxis)\n",
    "#         return X\n",
    "#     #print(X_S.shape)\n",
    "#     X=scaler.fit_transform(X)\n",
    "#     return X\n",
    "\n",
    "def scale(X,testing=False,mode='standard',a=None,b=None):\n",
    "    #X=np.nan_to_num\n",
    "    X=np.array(X)\n",
    "    if  mode=='scale':\n",
    "        if(not testing):\n",
    "            mx,mn=X.max(axis=0),X.min(axis=0)\n",
    "        else:\n",
    "            mx,mn=b,a\n",
    "        mx=np.where(mx==mn,mx+1,mx)\n",
    "        X=(X-mn)/(mx-mn)\n",
    "        if(testing):return X\n",
    "        return X,mn,mx\n",
    "    elif mode=='standard':\n",
    "        if(not testing):\n",
    "            mean,std=X.mean(axis=0),X.std(axis=0)\n",
    "        else:\n",
    "            mean,std=a,b\n",
    "        std=np.where(std==0,1,std)\n",
    "        X=(X-mean)/std\n",
    "        if(testing):return X\n",
    "        return X,mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotGraph(costs,fig_name,net=None,plot=True,Xtitle='Layer Count'):\n",
    "    #plt.plot(t, t, 'r--', t, t**2, 'bs', t, t**3, 'g^')\n",
    "    print('plott?',plot)\n",
    "    aa=list(costs.values())   \n",
    "    aa=np.array([list(i) for i in  aa])\n",
    "    a1,a2,a3,a4,a5=aa.T #accuracy, cost\n",
    "    plt.figure(num=None, figsize=(8, 6), dpi=600, facecolor='w', edgecolor='k')\n",
    "    #write after this line.\n",
    "    plt.ylabel(\"Accuracy/Cost<Scaled-down by max={0}>\".format(int(np.max(a2))))\n",
    "    \n",
    "    if type(net)==list:\n",
    "        plt.title('DataSet={0}, model={1}, part={2}, task={3}'.format(net[0],net[1],net[2],net[3]))\n",
    "        plt.xlabel(Xtitle)\n",
    "    elif net is not None:\n",
    "        print('yeah')\n",
    "        plt.title('Dataset={1}, Layers={3}, Costs={2},\\nActivators={0},batch_size={7}, ADAM={6}\\nWeight-Init={4}, alpha={5},distribution={8}'.\n",
    "                  format(net.activations,net.dataSetName,net.costName,net.layers,net.wInit,net.learningRate,net.doOp,net.batchSize,net.mode))\n",
    "        plt.xlabel('no. of epochs')\n",
    "    \n",
    "    plt.subplot().plot(list(costs.keys()),a1,'*',label='Accuracy on Train Set')\n",
    "    plt.subplot().plot(list(costs.keys()),a2/np.max(a2),'b', label='Cost of Train Data')\n",
    "    plt.subplot().plot(list(costs.keys()),a3,'b--', label='Accuracy on Validation Set')\n",
    "    plt.subplot().plot(list(costs.keys()),a4, label='f1-micro')\n",
    "    plt.subplot().plot(list(costs.keys()),a5, label='f1-macro')\n",
    "    \n",
    "    plt.legend(loc='best', shadow=False)\n",
    "\n",
    "    plt.savefig(fig_name)\n",
    "    if not plot:\n",
    "        pass\n",
    "        #mpl.use('Agg')\n",
    "    else:\n",
    "        %matplotlib inline\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot(y,gClasses=None):\n",
    "        S=list(set(y))\n",
    "        if (gClasses):\n",
    "            S=list(gClasses.values())\n",
    "        classes={}\n",
    "        #Y=np.zeros( (len(y),len(classes)))\n",
    "        for i in range(len(S)):\n",
    "            classes[i]=S[i]\n",
    "        Y=[ [0 for i in range(len(S)) ] for _ in range(len(y))]\n",
    "        for i in range(len(y)):\n",
    "            #print(i,classes.index(y[i]))\n",
    "            Y[i][S.index(y[i])]+=1\n",
    "            #print(Y[i],classes.index(y[i]),i)\n",
    "        if(gClasses):\n",
    "            return Y\n",
    "        return Y,classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X,y,dataSetName,path,mode='standard',doScale=True,testing=False,classes=None):\n",
    "        if(doScale):\n",
    "            if(not testing):\n",
    "                X,a,b=scale(X,testing,mode=mode)\n",
    "                np.save('{0}_MODEL/A'.format(path),a)\n",
    "                np.save('{0}_MODEL/B'.format(path),b)\n",
    "                y,classes=oneHot(y)\n",
    "                return X,y,classes\n",
    "            else:\n",
    "                a=np.load('{0}_MODEL/A.npy'.format(path)).tolist()\n",
    "                b=np.load('{0}_MODEL/B.npy'.format(path)).tolist()\n",
    "                X=scale(X,testing,'standard',a,b)\n",
    "                y=oneHot(y,classes)\n",
    "                return X,y\n",
    "    \n",
    "def BagOfWords(X,keys=None):\n",
    "    #Converts word matrix to n X D matrix.\n",
    "    #pre-process\n",
    "    if keys is None:\n",
    "        UniqueDict={}   \n",
    "        for a in X:\n",
    "            for t in a:\n",
    "                    if t not in UniqueDict:\n",
    "                            UniqueDict[t]=0\n",
    "                    UniqueDict[t]+=1\n",
    "        X_D=np.zeros((len(X),len(UniqueDict)),dtype='int32')\n",
    "        keys=list(UniqueDict.keys())\n",
    "        for a,c in zip(X, [ i for i in range(len(X))]):\n",
    "            for t in a:\n",
    "                    if t not in keys:\n",
    "                            continue#security check\n",
    "                    X_D[c][keys.index(t)]+=1\n",
    "\n",
    "        return X_D,keys\n",
    "    \n",
    "    #else:\n",
    "        X_D=np.zeros((len(X),len(keys)),dtype='int32')        \n",
    "        for a,c in zip(X, [ i for i in range(len(X))]):\n",
    "            for t in a:\n",
    "                    if t not in keys:\n",
    "                            continue#security check\n",
    "                    X_D[c][keys.index(t)]+=1\n",
    "        return X_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetwork:\n",
    "    def __init__(self,X,y,classes=None,oneHot=True,dataSetName=\"\",wInit=True,mode=\"gaussian\",diminish=1,\n",
    "                 hiddenlayers=[128,35],activations=['relu','tanh','soft-max'],cost='L2',\n",
    "                 learningRate=[0.1,0.01,0.001]):\n",
    "        self.dataSetName=dataSetName\n",
    "        self.weightInit=\"random\"\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        self.classes=classes\n",
    "        self.counter=0\n",
    "        self.y=np.array(self.y)\n",
    "        self.isOneHot=oneHot\n",
    "        self.wInit=wInit\n",
    "        self.mode=mode\n",
    "        self.myactivators={'sigmoid':self.sigmoid,\n",
    "                      'tanh':self.tanh,\n",
    "                      'soft-max':self.softmax,\n",
    "                        'relu':self.relu,\n",
    "                        'swish':self.swish\n",
    "                          }\n",
    "        self.mycosts={'L2':self.L2_cost, 'cross_entropy':self.cross_entropy}\n",
    "        self.hiddenlayers=np.array(hiddenlayers)\n",
    "        self.layers=list(hiddenlayers)\n",
    "        self.layers.insert(0,self.X.shape[1])\n",
    "        self.layers.append(self.y.shape[1])\n",
    "        print(self.layers)\n",
    "        self.activations=activations\n",
    "        print(activations)\n",
    "        self.methods=[ self.myactivators[i] for i in activations]\n",
    "        self.learningRate=learningRate\n",
    "        self.costName=cost\n",
    "        self.cost=self.mycosts[cost]\n",
    "        self.createLayers(diminish,self.wInit,self.mode)\n",
    "        #self.initBias()\n",
    "        self.initADAM()\n",
    "        self.initADAMbias()\n",
    "    def fitOnOtherDataSet(self,X,y,oneHot=True):\n",
    "        self.X=self.scale(np.array(X))\n",
    "        self.y=self.oneHot(y)\n",
    "    '''        \n",
    "    def dep_fit_train(X,y,self,batch_size=32,epochs=10):\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        n=len(y)\n",
    "        for epoch in range(epochs):\n",
    "            print(\"epoch:{0}\".format(epoch+1))\n",
    "            inx=0\n",
    "            while(inx<n):\n",
    "                if(inx+batch_size>n):\n",
    "                    Y=self.y[inx:]\n",
    "                    X=self.X[inx:]\n",
    "                else:    \n",
    "                    Y=self.y[inx:inx+batch_size]\n",
    "                    X=self.X[inx:inx+batch_size]\n",
    "                \n",
    "                self.train(X,Y)\n",
    "                inx+=batch_size\n",
    "            y_pred=self.getPredictions(X)\n",
    "            print(\"Accuracy:\",self.getAccuracy(self.y,y_pred))\n",
    "    def dep_train(self,X,y,itr=1000):\n",
    "        for _ in range(itr):\n",
    "            if(_%100==0):\n",
    "                print(\"training model at {0}th iteration\".format(_))\n",
    "            A,Z=self.feedForward(X,self.methods)\n",
    "            self.backprop(A,Z,self.methods,y,self.cost,len(X))\n",
    "    '''\n",
    "    def testModel(self,X,y):\n",
    "        yp=self.getPredictions(X)\n",
    "        y=self.getOriginalClassIndex(np.array(y))\n",
    "            \n",
    "        print(\"Accuracy::\",self.getAccuracy(yp,y))\n",
    "        mi,mn=self.getF1Scores(y,yp)\n",
    "        print(\"f1 Micro::\",mi)\n",
    "        print(\"f1 Macro::\",mn)\n",
    "        \n",
    "        \n",
    "    def getPredictions(self,X):\n",
    "        z=X\n",
    "        for i in range(len(self.layers)-1):\n",
    "            w=self.weights[i]\n",
    "            b=self.bias[i+1]#1xk\n",
    "            a=np.add( np.dot(z,w) , b) #mxn nxk= mxk  -- wx+b\n",
    "            z=self.methods[i](a)\n",
    "        yp=np.argmax(z,axis=1)\n",
    "        return yp\n",
    "    def getOriginalClassIndex(self,z):#getOriginalClassIndex\n",
    "        return np.argmax(z,axis=1)\n",
    "    def getAccuracy(self,y_1,y_2):#Classification !\n",
    "        return np.mean((y_1==y_2)) #CHECKPOINT, IF IT IS IN ONE HOT, THIS WILL LED WRONG RESULTS\n",
    "    def ADAM_main(self,count,i,alpha,grad,bgrad):\n",
    "        t=count+1\n",
    "        #print('t:',t)\n",
    "        self.weights[i],self.Am[i],self.As[i]=self.ADAM_updateWt(t,self.Am[i],self.As[i],self.weights[i],grad,alpha=alpha)\n",
    "        #print(\"i:\",i)\n",
    "        if(i==-1):\n",
    "                #print(self.adamM[i],self.adamM)\n",
    "                self.bias[i],self.adamM[i],self.adamS[i]=self.ADAM_updateBias(t,self.adamM[i],self.adamS[i],self.bias[i],bgrad,alpha=alpha)\n",
    "        else:\n",
    "            self.bias[i+1],self.adamM[i+1],self.adamS[i+1]=self.ADAM_updateBias(t,self.adamM[i+1],self.adamS[i+1],self.bias[i+1],bgrad,alpha=alpha)\n",
    "        \n",
    "    def ADAM_WU(self,m,s,weight,grad,beta1=0.9,beta2=0.999,alpha=0.001,epsilon=1e-8):\n",
    "        pass \n",
    "    \n",
    "    def ADAM_updateWt(self,t,m,s,weight,grad,beta1=0.9,beta2=0.999,alpha=0.001,epsilon=1e-8):\n",
    "        #print('aupwd:',t,np.max(s))\n",
    "        m=beta1*m+(1-beta1)*grad\n",
    "        s=beta2*s+(1-beta2)*np.multiply(grad,grad)\n",
    "        mx=m/((1-beta1**t) )\n",
    "        sx=s/(1-beta2**t)\n",
    "        weight1=weight-alpha* np.divide(mx, (sx+epsilon)**(0.5) )\n",
    "        #print(weight1==weight)\n",
    "        return weight1,m,s\n",
    "    \n",
    "    def ADAM_updateBias(self,t,m,s,weight,grad,beta1=0.9,beta2=0.999,alpha=0.001,epsilon=1e-8):\n",
    "        m=np.array(m)\n",
    "        grad=np.array(grad)\n",
    "        #print(beta1,m.shape,grad.shape)\n",
    "        m=beta1*m+(1-beta1)*grad\n",
    "        s=beta2*s+(1-beta2)*np.multiply(grad,grad)\n",
    "        #print(beta1,t,beta1**t,m)\n",
    "        mx=m/((1-beta1**t) )\n",
    "        sx=s/(1-beta2**t )\n",
    "        weight1=weight-alpha* np.divide(mx, (sx+epsilon)**(0.5) )\n",
    "        #print(weight1==weight)\n",
    "        return weight1,m,s\n",
    " \n",
    "    def xav(self,L,K):\n",
    "        return np.random.randn(L,K)*np.sqrt(1/L)\n",
    "    def he(self,L,K):\n",
    "        return np.random.randn(L,K)*np.sqrt(6/(L+K))\n",
    "    \n",
    "    def initWB(self,IP,OP,activator='relu',He=True,mode='gaussian'):\n",
    "        print(IP,OP,activator)\n",
    "        if He:\n",
    "            _ = 1/(IP+OP)**0.5\n",
    "            if activator in ('sigmoid','soft-max'):\n",
    "                r, s = 6**0.5, 2**0.5\n",
    "            elif activator=='tanh':\n",
    "                r, s = 4*6**0.5, 4*2**0.5\n",
    "            else: # relu or swish function\n",
    "                r, s = 12**0.5, 2\n",
    "            r, s = r*_, s*_\n",
    "        else:\n",
    "            r, s = 1, 1\n",
    "        # Generating matrices\n",
    "        if mode=='uniform':\n",
    "            print('Mode -- Uniform')\n",
    "            return 2*r*np.random.random((IP,OP))-r , 2*r*np.random.random((1,OP))-r\n",
    "        elif mode=='gaussian':\n",
    "            print('Mode -- gaussian')\n",
    "            return np.random.randn(IP,OP)*s , np.random.randn(1,OP)*s\n",
    "        else:\n",
    "            print('Mode -- zeros')\n",
    "            return np.zeros((IP,OP))\n",
    "\n",
    "    \n",
    "    def createLayers(self,diminish=1e0,He=True,mode='gaussian'):\n",
    "        self.weights=[]\n",
    "        self.bias=[[]]\n",
    "        \n",
    "        for i in range(len(self.layers)-1):\n",
    "            #print(self.layers[i],self.layers[i+1],self.activations[i])\n",
    "            w,b=self.initWB(self.layers[i],self.layers[i+1],self.activations[i],He=He,mode=mode)\n",
    "            #self.weights.append( np.random.rand(self.layers[i],self.layers[i+1]) *diminish)\n",
    "            #self.weights.append( np.zeros((self.layers[i],self.layers[i+1])))\n",
    "            self.weights.append(w)\n",
    "            self.bias.append(np.array(b))\n",
    "        print('size of bias:',len(self.bias))\n",
    "        \n",
    "    def initADAM(self):\n",
    "        self.Am=[]\n",
    "        self.As=[]\n",
    "        for i in range(len(self.layers)-1):\n",
    "            \n",
    "            #self.weights.append( np.random.rand(self.layers[i],self.layers[i+1]) *diminish)\n",
    "            self.Am.append( np.zeros((self.layers[i],self.layers[i+1])))\n",
    "            self.As.append( np.zeros((self.layers[i],self.layers[i+1])))\n",
    "    def initADAMbias(self):\n",
    "        self.adamM=[[]]\n",
    "        for i in range(1,len(self.layers)):\n",
    "            self.adamM.append( np.zeros((1,self.layers[i])))\n",
    "        #print('size of bias:',len(self.adamM))\n",
    "        self.adamS=[[]]\n",
    "        for i in range(1,len(self.layers)):\n",
    "            self.adamS.append( np.zeros((1,self.layers[i])))\n",
    "        #print('size of bias:',len(self.adamS))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def initBias(self):\n",
    "        self.bias=[[]]\n",
    "        for i in range(1,len(self.layers)):\n",
    "            self.bias.append( np.zeros((1,self.layers[i])))\n",
    "        print('size of bias:',len(self.bias))\n",
    "    def getF1Scores(self,aa,bb):\n",
    "        micron=sklearn.metrics.f1_score(aa,bb,average='micro')\n",
    "        macron=sklearn.metrics.f1_score(aa,bb,average='macro')\n",
    "        return micron, macron\n",
    "            \n",
    "    \n",
    "    def train(self,initADAMS=True,doOp=False,batch_size=1,KKK=1,epochs=500,earlyStopping=False,X_val=None,y_val=None,printResults=False,minEpochs=100,patience=10): \n",
    "        #y_val== onehot vector.\n",
    "        self.doOp=doOp\n",
    "        acc_val,acc_main=0,1e100\n",
    "        isUP=False #CHECKPOINT, m bola SGD CHALANE, TUM ADAM CHALAKE MAANOGE\n",
    "        Costs={}\n",
    "        n=self.X.shape[0]\n",
    "        yp_ind=self.getOriginalClassIndex(self.y)\n",
    "        self.batchSize=batch_size\n",
    "        if(earlyStopping):\n",
    "            X_val=np.array(X_val)\n",
    "            #print(y_val)\n",
    "            y_val=self.getOriginalClassIndex(np.array(y_val))\n",
    "            #print(y_val)\n",
    "            \n",
    "        for _ in range(epochs):\n",
    "            \n",
    "            start_time=datetime.datetime.now()\n",
    "            if(initADAMS):\n",
    "                self.initADAM()\n",
    "                self.initADAMbias()\n",
    "            self.counter+=1\n",
    "            if(printResults and (self.counter)%KKK==0):print((self.counter),end=' ')\n",
    "            cost=0\n",
    "            inx=0\n",
    "            count=0\n",
    "            while(inx<n):\n",
    "                count+=1\n",
    "                if(inx+batch_size>n):\n",
    "                    Y_=self.y[inx:]\n",
    "                    X_=self.X[inx:]\n",
    "                else:    \n",
    "                    Y_=self.y[inx:inx+batch_size]\n",
    "                    X_=self.X[inx:inx+batch_size]\n",
    "                \n",
    "                A,Z=self.feedForward(np.array(X_),self.methods)\n",
    "                cost+=self.backprop(A,Z,_,self.methods,np.array(Y_),self.cost,returnCost=True,doOp=doOp)\n",
    "                inx+=batch_size\n",
    "            \n",
    "            if(earlyStopping):\n",
    "                y_val_pred=self.getPredictions(X_val)\n",
    "                #print(y_val_pred)\n",
    "                tmp=self.getAccuracy(y_val,y_val_pred)\n",
    "                if(isUP and tmp<acc_val and self.counter>minEpochs):\n",
    "                    if(patience==10):\n",
    "                        self.saveModel('{0}_patience_at_{1}'.format(self.dataSetName,self.counter))\n",
    "                        #np.save('{0}_patience_0'.format(self.dataSetName))\n",
    "                    patience-=1\n",
    "                    if(patience==0):\n",
    "                        break\n",
    "                if(tmp>acc_val):\n",
    "                    acc_val=tmp\n",
    "                    isUP=True            \n",
    "                    \n",
    "            y_pred=self.getPredictions(self.X)\n",
    "            acc_main=self.getAccuracy(yp_ind,y_pred)\n",
    "            #acc_main=np.mean(y_pred==self.y_orig.T)\n",
    "            mi,ma=self.getF1Scores(yp_ind,y_pred)\n",
    "            \n",
    "            end_time=datetime.datetime.now()\n",
    "            if(printResults and(self.counter)%KKK==0):  \n",
    "                \n",
    "                \n",
    "                print(\"Cost:\",cost,\"acc:\",acc_main, 'validation_acc:',acc_val\n",
    "                     ,'micro:',mi,'macro:',ma,'time:',end_time-start_time)\n",
    "                \n",
    "                pass\n",
    "            if(earlyStopping):\n",
    "                Costs[self.counter]=[acc_main,cost,acc_val,mi,ma]\n",
    "            else:\n",
    "                Costs[self.counter]=[acc_main,cost]\n",
    "        return Costs\n",
    "    \n",
    "    def saveModel(self,modelName):\n",
    "        np.save(modelName,[self.counter,self.weights,self.bias,self.activations,self.learningRate,self.layers,self.classes,self.costName])\n",
    "    \n",
    "    \n",
    "    def loadModel(self,modelName):\n",
    "        self.counter,self.weights,self.bias,self.activations,self.learningRate,self.layers,self.classes,self.costName=np.load(modelName).tolist()\n",
    "        self.methods=[ self.myactivators[i] for i in self.activations]\n",
    "        self.cost=self.mycosts[self.costName]\n",
    "        \n",
    "    def feedForward(self,X,method):\n",
    "        '''\n",
    "        Note X-- nxd -- represents n= images with d dim.\n",
    "        W[0]=layer[0] X layer[1] or d x l1\n",
    "        so a[1]= np.dot(X, W[0])\n",
    "        z[1]=activator(a[1]) can be sigmoid/relu/tanh/squish etc...\n",
    "        '''\n",
    "        Z=[X]\n",
    "        A=[[]]\n",
    "        for i in range(len(self.layers)-1):\n",
    "            w=self.weights[i]#nxk  #YAHAN TU BIAS & WEIGHT K ALAG INDEX LIYE HO, but BACK_PROP m SAME, CHAKKAR kya h\n",
    "            b=self.bias[i+1]#1xk\n",
    "            a=np.add( np.dot(Z[i],w) , b) #mxn nxk= mxk  -- wx+b\n",
    "            A.append(np.array(a))\n",
    "            z=method[i](a)\n",
    "            Z.append(np.array(z))\n",
    "            \n",
    "            #print(\"A Z shape\",A[-1].shape,Z[-1].shape)\n",
    "        return A,Z\n",
    "    \n",
    "    def backprop(self,A,Z,count,method,y,cost,optimizer=ADAM_main,printCost=False,returnCost=True,doOp=True):\n",
    "        #here it should be no. of samples--batch size\n",
    "        #print(\"z,y,shapes\",Z[-1].shape,y.shape)\n",
    "        m=Z[0].shape[0]\n",
    "        E=cost(Z[-1],y)\n",
    "        if(printCost):\n",
    "            print(\"COST:\",E)\n",
    "        dEdOout=cost(Z[-1],y,derivative=True)# CHECKPOINT, why so 1D-vector. Actually its m X 1d-vector\n",
    "        dOoutdOin=method[-1](A[-1],derivative=True)#1D-vector\n",
    "        dOindw=Z[-2]#HlastOut 1D-vector\n",
    "        #print(\"dOindw nx14\",dOindw.shape)\n",
    "        #####\n",
    "        dEdOin=dEdOout*dOoutdOin#This is right  \n",
    "        #print('dEdOin shape',dEdOin.shape)\n",
    "        '''\n",
    "        n=1\n",
    "        dEdw=np.matmul(dOindw.reshape(-1,n),dEdOin.reshape(n,-1)) # (Hlast,n)* (n,Oin) -- can cause problem for batch-grad\n",
    "        '''\n",
    "        dEdw=np.dot(dOindw.T,dEdOin) # (Hlast,n)* (n,Oin) -- can cause problem for batch-grad\n",
    "        #print('dedw shape',dEdw.shape)\n",
    "        if(doOp):\n",
    "            dEdw=dEdw/np.where(np.mean(dEdw)==0,1,np.mean(dEdw))\n",
    "        \n",
    "            optimizer(self,count,-1,self.learningRate[-1],dEdw,np.mean(dEdOin,axis=0))#sum?\n",
    "        else:\n",
    "            self.weights[-1]-=self.learningRate[-1]*(dEdw/m)\n",
    "        \n",
    "            self.bias[-1]-=self.learningRate[-1]*np.mean(dEdOin,axis=0)\n",
    "        #print('dedw:{0}\\ndEdOin:{1}\\ndEdOout:{2}\\ndOoutdOin:{3}'.format(dEdw,dEdOin,dEdOout,dOoutdOin))\n",
    "        #### Do general Recursion Now.\n",
    "        #Call dEdOin as delta\n",
    "        delta= dEdOin\n",
    "        #print('delta:',delta.shape)\n",
    "          \n",
    "        # Weights=[in * h1, h1 *h2, h2 * hlast, hlast * out]\n",
    "        # Already Calculated hlast* out or weights[-1]\n",
    "        for i in range(len(self.weights)-2,-1,-1):\n",
    "            '''\n",
    "            size(Z)=size(A)=size(weights)+1\n",
    "            '''\n",
    "            dHoutdHin=method[i](A[i+1],derivative=True)\n",
    "            dHindw=Z[i]\n",
    "            #dHindw=np.tile(dHindw.reshape(-1,1),self.weights[i].shape[1])\n",
    "            #print('dhindw',dHindw)\n",
    "            #Need to find dEtotaldHout=dEtotal_dOin*dOin_dHout\n",
    "            dEtotaldHout=np.dot(delta,self.weights[i+1].T)\n",
    "            #print()\n",
    "            dEdHin=np.multiply(dEtotaldHout,dHoutdHin)     #refraining use of Etotal. jUst E now. \n",
    "            #print(\"e/hout\",dEtotaldHout,\"\\nhout/hin\",dHoutdHin,\"\\ne/hin\",dEdHin)\n",
    "            dEdw=np.dot(dHindw.T,dEdHin) # (Hlast,1)* (1,Oin)\n",
    "            #print(dEdw.shape,dEdw)\n",
    "            if(doOp):\n",
    "                dEdw=dEdw/np.where(np.mean(dEdw)==0,1,np.mean(dEdw))\n",
    "                optimizer(self,count,i,self.learningRate[i],dEdw,np.mean(dEdHin,axis=0))\n",
    "            else:\n",
    "                self.weights[i]-=self.learningRate[i]*(dEdw/m)\n",
    "            \n",
    "                self.bias[i+1]-=self.learningRate[i]*np.mean(dEdHin,axis=0)\n",
    "            delta=dEdHin\n",
    "            #print('delta:',delta)\n",
    "        return np.mean(E)\n",
    "            \n",
    "        '''\n",
    "        np.repeat(z,3,axis=0).reshape(3,3)-x\n",
    "        try to make it for mini-batch over stochastic\n",
    "        '''\n",
    "        \n",
    "    def softmax(self,a,derivative=False):\n",
    "        z=np.exp(a-a.max(axis=1,keepdims=True))\n",
    "        if(derivative):\n",
    "            su=np.sum(z,axis=1).reshape(-1,1)#try to use np.sum(s,axis=1 for row-wise sum ; 0 for col-wise sum)\n",
    "            t=su-z\n",
    "            tsq=np.sum(z,axis=1).reshape(-1,1)**2\n",
    "            z=np.multiply(t,z)\n",
    "            return z/np.maximum(tsq,1e-6)\n",
    "        return z/np.sum(z,axis=1).reshape(-1,1)\n",
    "    \n",
    "    def relu(self,a,derivative=False):\n",
    "        if(derivative==True):\n",
    "            return (np.sign(a)>0)*1\n",
    "        \n",
    "        return np.maximum(a,0)\n",
    "    def swish(self,a,derivative=False):\n",
    "        z=a* self.sigmoid(a)\n",
    "        if derivative:\n",
    "            z=z+self.sigmoid(a)*(1-z)\n",
    "        return z\n",
    "    def sigmoid(self,a,derivative=False):\n",
    "        \n",
    "        #z= np.array(1/(1+ np.exp(np.multiply(a,-1))) )\n",
    "        #try:\n",
    "        # Prevent overflow.\n",
    "        a = np.clip( a, -500, 500 )\n",
    "        f = lambda x: 1/(1+np.exp(-x))\n",
    "        g = lambda x: np.exp(x)/(1+np.exp(x))\n",
    "        z= np.where(a>=0,f(a),g(a))\n",
    "#         if(a.any()>=0):\n",
    "#             z= np.array(1/(1+ np.exp(-a)) )\n",
    "#         else:\n",
    "#             z= np.array(1/(1+ np.exp(a)) )\n",
    "# #         except:\n",
    "# #             print('Sigmoid error:{0} at epoch:{1} layer:{2}'.format(np.max(-a),self.counter,a.shape))\n",
    "# #             z=a\n",
    "        if(derivative ==True):\n",
    "            return np.multiply(z,(1-z))\n",
    "        return z\n",
    "    def tanh(self,a,derivative=False):\n",
    "        #z=(2/(1+np.exp(-2*a))) -1\n",
    "        if(derivative):\n",
    "             return (1 - (np.tanh(a) ** 2)) \n",
    "        return np.tanh(a)\n",
    "    \n",
    "    def Identity(self,a,derivative=False):\n",
    "        return a\n",
    "    \n",
    "    def L2_cost(self,A,B,derivative=False):\n",
    "        A=np.array(A)#OUT\n",
    "        B=np.array(B)#Actual output y\n",
    "        #print('cost:',A.shape, B.shape)\n",
    "        C=A-B\n",
    "        if(derivative):\n",
    "            return C\n",
    "        return np.sum(C**2,axis=1)\n",
    "    \n",
    "    def cross_entropy(self,CalcOutput,trueOutput,derivative=False):\n",
    "        \n",
    "        A=np.array(CalcOutput)#OUT\n",
    "        B=np.array(trueOutput)#Actual output y\n",
    "        A=np.where(B!=1,A+np.e,A)# 0log0\n",
    "        A=np.where(np.logical_and(B==1,A==0),A+1e-8,A)#1log0\n",
    "        #print('cost:',A.shape, B.shape)\n",
    "        if(derivative):\n",
    "            return A-B\n",
    "        return np.sum(-1*B*(np.log(A)),axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alexnet.py\n",
    "\n",
    "\"\"\" AlexNet.\n",
    "References:\n",
    "    - Alex Krizhevsky, Ilya Sutskever & Geoffrey E. Hinton. ImageNet\n",
    "    Classification with Deep Convolutional Neural Networks. NIPS, 2012.\n",
    "Links:\n",
    "    - [AlexNet Paper](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n",
    "\"\"\"\n",
    "\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "\n",
    "def alexnet(width, height, lr,targets):\n",
    "    network = input_data(shape=[None, width, height, 1], name='input')\n",
    "    network = conv_2d(network, 96, 11, strides=4, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 256, 5, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    network = local_response_normalization(network)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 4096, activation='tanh',name='embed')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, targets, activation='softmax')\n",
    "    network = regression(network, optimizer='momentum',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         learning_rate=lr, name='targets')\n",
    "\n",
    "    model = tflearn.DNN(network, checkpoint_path='model_alexnet',\n",
    "                        max_checkpoints=1, tensorboard_verbose=0, tensorboard_dir='log')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WIDTH=32\n",
    "HEIGHT=32\n",
    "learningRate=0.01#RGB\n",
    "targets=10\n",
    "X_train =np.array( [cv2.cvtColor(x_, cv2.COLOR_BGR2GRAY) for x_ in x_train[:2000]]).reshape(-1,WIDTH,HEIGHT,1)\n",
    "X_test =np.array( [cv2.cvtColor(x_, cv2.COLOR_BGR2GRAY) for x_ in x_test[:1000]]).reshape(-1,WIDTH,HEIGHT,1)\n",
    "print(X_train[0].shape, 'train samples')\n",
    "print(X_test.shape, 'test samples')\n",
    "\n",
    "model=alexnet(WIDTH,HEIGHT,COLORS,targets)\n",
    "MODEL_NAME='AlexNet_CNN'\n",
    "\n",
    "model.fit({'input': X_train[:2000]}, {'targets': y_train[:2000]}, n_epoch=1, validation_set=({'input':X_test[:1000]}, {'targets': y_test[:1000]}), \n",
    "             snapshot_step=500, show_metric=True, run_id=MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='RMSprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "data_augmentation=False\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet(folder,rgb=False,itr=None):\n",
    "    images = []\n",
    "    count=0\n",
    "    for filename in os.listdir(folder):\n",
    "        count+=1\n",
    "        #img=scipy.ndimage.imread(os.path.join(folder, filename), mode='L')\n",
    "        img = (mpimg.imread(os.path.join(folder, filename)))\n",
    "        if(rgb):\n",
    "            img=rgb2gray(img)\n",
    "            \n",
    "        img=np.array(img)\n",
    "        #img=img.reshape(-1,1)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "        if itr is not None and count>itr:\n",
    "            break\n",
    "        \n",
    "    return images\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "    \n",
    "def loadDataSet2(folder,IMG_SIZE=100,as_gray=True,itr=None):\n",
    "    images = []\n",
    "    count=0\n",
    "    for filename in os.listdir(folder):\n",
    "        count+=1\n",
    "        #img=scipy.ndimage.imread(os.path.join(folder, filename), mode='L')\n",
    "        #img = io.imread(os.path.join(folder, filename),as_gray=as_gray)\n",
    "        img = cv2.imread(os.path.join(folder, filename),cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))    \n",
    "        img=np.array(img)\n",
    "        #img=img.reshape(-1,1)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "        if itr is not None and count>itr:\n",
    "            break\n",
    "        \n",
    "    return images\n",
    "\n",
    "def LoadDataForCSV(fileName):\n",
    "    f=open(fileName,'r')\n",
    "    X=np.array([[float(i) for i in line.split(' ')] for line in f])\n",
    "    f.close()\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "def LoadDataForTXT(fileName):\n",
    "    f=open(fileName,'r')\n",
    "    X=np.array([[str(i) for i in line.strip().split(' ')] for line in f])\n",
    "    f.close()\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#X_train,y_train,X_val,y_val,classes=MNIST()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.shape,y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.min(X_train),np.max(X_train,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#X_train,y_train,X_val,y_val,classes=catdog(28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#X_train,y_train,X_val,y_val,classes=Dolphins()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#X_train,y_train,X_val,y_val,classes=Pubmed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp=list(map(tuple,y_val))\n",
    "# A=[]\n",
    "# print('val')\n",
    "# for i in set(tmp):\n",
    "#     A.append( tmp.count(i))\n",
    "#     print(i, tmp.count(i)/np.sum(tmp))\n",
    "# tmp=list(map(tuple,y_train))\n",
    "# B=[]\n",
    "# print('train')\n",
    "# for i in set(tmp):\n",
    "#     B.append( tmp.count(i))\n",
    "#     print(i, tmp.count(i)/np.sum(tmp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TASK_1(dataSetName,X_train,y_train,X_val,y_val,classes,layers,words=None,maxEpochs=100,minLayerSize=1500):\n",
    "    m=X_train.shape[1]\n",
    "    l=y_train.shape[1]\n",
    "    results={}\n",
    "    \n",
    "    Activators=['sigmoid','tanh','relu','swish']\n",
    "    \n",
    "    #sizes=[2**i for i in range( 2*round(np.log2(16)) )  ]\n",
    "    alphas=[10**(-i) for i in range(0,layers+1)]\n",
    "    gmi=np.minimum(m//2,minLayerSize)\n",
    "    HLList=[]\n",
    "    AList=[]\n",
    "    LRList=[]\n",
    "    FULL_TEST=[[],[],[],[],[]]\n",
    "    counter=1\n",
    "    for layer in range(1,layers+1):\n",
    "            tacc=0\n",
    "            tbstHL=None\n",
    "            tbstAL=None\n",
    "            tbstLR=None\n",
    "            tbstCost=None\n",
    "            for i in Activators:\n",
    "                start_time=datetime.datetime.now()\n",
    "            \n",
    "                tmpHL=HLList+[(gmi)]\n",
    "                tmpAL=AList+[(i)]\n",
    "                tmpLR=LRList+[(alphas[layer])]\n",
    "                #[gmi,ac,alphas[layer]]   \n",
    "                print()\n",
    "                print('tmphl:',tmpHL,'tmpal:',tmpAL,'tmplr:',tmpLR)\n",
    "                try:\n",
    "                    net=neuralNetwork(np.array(X_train),np.array(y_train),classes,dataSetName=dataSetName,wInit=\"he\",mode='gaussian' ,hiddenlayers=tmpHL,activations=tmpAL+['soft-max'],cost='L2',learningRate=tmpLR+[0.0003])#mnist\n",
    "                    costs=(net.train(initADAMS=False,batch_size=1000,doOp=False,epochs=maxEpochs,KKK=1,earlyStopping=True,X_val=X_val,y_val=y_val,minEpochs=1,patience=0))   \n",
    "                    path='plots/{0}/TASK1/{0}_{1}.png'.format(dataSetName,counter)\n",
    "                    counter+=1\n",
    "                    plotGraph(costs,path,net,plot=False)\n",
    "                    tmpCost=costs[max(costs)]\n",
    "                    if(tacc<tmpCost[0]):\n",
    "                        tbstHL=tmpHL\n",
    "                        tbstAL=tmpAL\n",
    "                        tbstLR=tmpLR\n",
    "                        tacc=tmpCost[0]\n",
    "                        tbstCost=tmpCost\n",
    "                    FULL_TEST[layer].append([tmpHL,tmpAL,tmpLR,tmpCost] )\n",
    "                except:\n",
    "                    print('Ignoring case:',tmpHL,tmpAL,tmpLR)\n",
    "                end_time=datetime.datetime.now()\n",
    "                print('time taken for {0} is {1}'.format(tmpHL,end_time-start_time))\n",
    "            gmi=int(round((gmi*l)**(0.5)))\n",
    "            results[layer]=tbstCost\n",
    "            HLList=tbstHL\n",
    "            AList=tbstAL\n",
    "            LRList=tbstLR\n",
    "    \n",
    "    return FULL_TEST,results,[HLList,AList,LRList]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TASK_2(dataSetName,X_train,y_train,X_val,y_val,classes,\n",
    "                params,layers=3,words=None,maxEpochs=100,minLayerSize=2000):\n",
    "    m=X_train.shape[1]\n",
    "    l=y_train.shape[1]\n",
    "    HLList,AList,LRList=params\n",
    "    bs=[]\n",
    "    print(HLList,'\\n',AList,'\\n',LRList)\n",
    "    sizes=[2**i for i in range( int(round(np.log2(np.minimum(m,minLayerSize)))))]\n",
    "    print(sizes)\n",
    "    bestHL=[]\n",
    "    bestACC=0\n",
    "    bestLR=[]\n",
    "    for hlayer in range(1,layers+1):\n",
    "        print('Hidden layer:',hlayer)\n",
    "        al=AList[0:hlayer]\n",
    "        lr=LRList[0:hlayer]\n",
    "        tbstHL=[]\n",
    "        results={}\n",
    "        tacc=0\n",
    "        tbstLR=[]\n",
    "        print('activations:',al)\n",
    "        for size in sizes:\n",
    "            tmpHL=bs+[size]\n",
    "            start_time=datetime.datetime.now()\n",
    "            \n",
    "            print('tmphl:',tmpHL)\n",
    "            try:\n",
    "                net=neuralNetwork(np.array(X_train),np.array(y_train),classes,dataSetName=dataSetName,wInit=\"he\",mode='gaussian',hiddenlayers=tmpHL,activations=al+['soft-max'],cost='L2',learningRate=lr+[0.0003])#mnist\n",
    "\n",
    "                #plotGraph(net,path,plot=False)\n",
    "                costs=(net.train(initADAMS=False,batch_size=1000,doOp=False,epochs=maxEpochs,KKK=1,earlyStopping=True,X_val=X_val,y_val=y_val,minEpochs=1,patience=0))   \n",
    "                tmpCost=costs[max(costs)]\n",
    "                if(tacc<tmpCost[0]):\n",
    "                    tbstHL=tmpHL\n",
    "                    tbstLR=lr\n",
    "                    tacc=tmpCost[0]\n",
    "                results[size]=tmpCost\n",
    "            except:\n",
    "                print('Ignoring case:',tmpHL,al,lr)\n",
    "            end_time=datetime.datetime.now()\n",
    "            print('time taken for {0} is {1}'.format(tmpHL,end_time-start_time))\n",
    "        path='plots/{0}/TASK2/{0}_Layer_{1}.png'.format(dataSetName,hlayer)\n",
    "        plotGraph(results,path,[dataSetName,'My_NN',2,hlayer],plot=False,Xtitle='neurons')\n",
    "        bs=tbstHL\n",
    "        if(bestACC<tacc):\n",
    "            bestHL=tbstHL\n",
    "            bestLR=tbstLR\n",
    "            \n",
    "    return bestHL\n",
    "    #Cases=[ [ ['relu'] ]]\n",
    "    #for element in itertools.product(*Cases):\n",
    "    #print(element)\n",
    "    #plotGraph(costs,fig_name,net=None,plot=True,Xtitle='Layer Count'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TASK_3(dataSetName,X_train,y_train,X_val,y_val,classes,\n",
    "                HLList,words=None,maxEpochs=100):\n",
    "    m=X_train.shape[1]\n",
    "    l=y_train.shape[1]\n",
    "    hlayers=len(HLList)\n",
    "    alphas=[10**(-i) for i in range(1,hlayers+1)]\n",
    "    print('Config:',HLList)\n",
    "    bestAL=[]\n",
    "    tacc=0\n",
    "    Cases=['relu','sigmoid','swish','tanh']\n",
    "    for element in itertools.product(Cases,repeat=hlayers):\n",
    "        al=list(element)  \n",
    "        results={}\n",
    "        #costs=[]\n",
    "        #net=None\n",
    "        start_time=datetime.datetime.now()\n",
    "        print('activations:',al)\n",
    "        try:\n",
    "            net=neuralNetwork(np.array(X_train),np.array(y_train),classes,dataSetName=dataSetName,wInit=\"he\",mode='gaussian',hiddenlayers=HLList,activations=al+['soft-max'],cost='L2',learningRate=alphas+[0.0003])#mnist\n",
    "            #plotGraph(net,path,plot=False)\n",
    "            costs=(net.train(initADAMS=False,batch_size=1000,doOp=False,epochs=maxEpochs,KKK=1,earlyStopping=True,X_val=X_val,y_val=y_val,minEpochs=1,patience=0))   \n",
    "            tmpCost=costs[max(costs)]\n",
    "\n",
    "        except:\n",
    "           print('Ignoring case:')\n",
    "        end_time=datetime.datetime.now()\n",
    "        print('time taken for {0} is {1}'.format(al,end_time-start_time))       \n",
    "        path='plots/{0}/TASK3/{0}_Layer_{1}.png'.format(dataSetName,al)\n",
    "        plotGraph(costs,path,net,plot=False)\n",
    "        if(tacc<tmpCost[0]):\n",
    "            tacc=tmpCost[0]\n",
    "            bestAL=al\n",
    "            \n",
    "\n",
    "    return bestAL\n",
    "    #Cases=[ [ ['relu'] ]]\n",
    "    #for element in itertools.product(*Cases):\n",
    "    #print(element)\n",
    "    #plotGraph(costs,fig_name,net=None,plot=True,Xtitle='Layer Count'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Task_4(dataSetName,X_train,y_train,X_val,y_val,classes,\n",
    "                bestParams,words=None,maxEpochs=100):\n",
    "    m=X_train.shape[1]\n",
    "    l=y_train.shape[1]\n",
    "    al,HLList=bestParams\n",
    "    hlayers=len(HLList)\n",
    "    alphas=[10**(-i) for i in range(1,hlayers+1)]\n",
    "    \n",
    "    net=neuralNetwork(np.array(X_train),np.array(y_train),classes,dataSetName=dataSetName,wInit=True,mode='gaussian',hiddenlayers=HLList,activations=al+['soft-max'],cost='L2',learningRate=alphas+[0.0003])#mnist\n",
    "    costs=(net.train(initADAMS=False,batch_size=1000,doOp=False,epochs=maxEpochs,KKK=1,earlyStopping=True,X_val=X_val,y_val=y_val,minEpochs=1,patience=0))   \n",
    "            \n",
    "    path='output_plots/{0}/TASK4/{0}_{1}.png'.format(dataSetName,[net.wInit,net.mode])\n",
    "        \n",
    "    plotGraph(costs,path,net,plot=False)\n",
    "        \n",
    "    net2=neuralNetwork(np.array(X_train),np.array(y_train),classes,dataSetName=dataSetName,wInit=True,mode='uniform',hiddenlayers=HLList,activations=al+['soft-max'],cost='L2',learningRate=alphas+[0.0003])#mnist\n",
    "    costs=(net2.train(initADAMS=False,batch_size=1000,doOp=False,epochs=maxEpochs,KKK=1,earlyStopping=True,X_val=X_val,y_val=y_val,minEpochs=1,patience=0))   \n",
    "            \n",
    "    path='output_plots/{0}/TASK4/{0}_{1}.png'.format(dataSetName,[net2.wInit,net2.mode])\n",
    "    \n",
    "    plotGraph(costs,path,net2,plot=False)\n",
    "        \n",
    "    net3=neuralNetwork(np.array(X_train),np.array(y_train),classes,dataSetName=dataSetName,wInit=False,mode='gaussian',hiddenlayers=HLList,activations=al+['soft-max'],cost='L2',learningRate=alphas+[0.0003])#mnist\n",
    "    costs=(net3.train(initADAMS=False,batch_size=1000,doOp=False,epochs=maxEpochs,KKK=1,earlyStopping=True,X_val=X_val,y_val=y_val,minEpochs=1,patience=0))   \n",
    "            \n",
    "    path='output_plots/{0}/TASK4/{0}_{1}.png'.format(dataSetName,[net3.wInit,net3.mode])\n",
    "    \n",
    "    plotGraph(costs,path,net3,plot=False)\n",
    "        \n",
    "    net4=neuralNetwork(np.array(X_train),np.array(y_train),classes,dataSetName=dataSetName,wInit=False,mode='uniform',hiddenlayers=HLList,activations=al+['soft-max'],cost='L2',learningRate=alphas+[0.0003])#mnist\n",
    "    costs=(net4.train(initADAMS=False,batch_size=1000,doOp=False,epochs=maxEpochs,KKK=1,earlyStopping=True,X_val=X_val,y_val=y_val,minEpochs=1,patience=0))   \n",
    "            \n",
    "    path='output_plots/{0}/TASK4/{0}_{1}.png'.format(dataSetName,[net4.wInit,net4.mode])\n",
    "    \n",
    "    plotGraph(costs,path,net4,plot=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doTASKS(X_train,y_train,X_val,y_val,classes,I=2,dataSetName=\"cat-dog\",mx=20,whichModel=\"MY_NN\"):\n",
    "    print('doing task1')\n",
    "    \n",
    "    full,task1res,bestParams=TASK_1(dataSetName,X_train,y_train,X_val,y_val,classes,3,words=None,maxEpochs=mx)\n",
    "\n",
    "    fig_name='output_plots/{0}/{0}_{1}.png'.format(dataSetName,'RESULT')\n",
    "    plotGraph(task1res,fig_name,[dataSetName,whichModel,I,1],plot=False)\n",
    "    #'DataSet={0}, model={1}, part={2}, task={3}'\n",
    "    #print('hello worlds')\n",
    "    print('Best parameter from task1s:',bestParams)\n",
    "    print('Doing task2')\n",
    "    bestHL=TASK_2(dataSetName,X_train,y_train,X_val,y_val,classes,bestParams,3,words=None,maxEpochs=mx)\n",
    "    print('Best Hiddenlayer from task2s:',bestHL)\n",
    "    print('Doing task3')\n",
    "    bestACC=TASK_3(dataSetName,X_train,y_train,X_val,y_val,classes,bestHL,words=None,maxEpochs=mx)\n",
    "    print('bestACC,bestHL',bestACC,bestHL)  \n",
    "    print('Doing Task4')\n",
    "    Task_4(dataSetName,X_train,y_train,X_val,y_val,classes,[bestACC,bestHL],words=None,maxEpochs=mx)\n",
    "    print('MY_NN tasks Complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doTASKS(X_train,y_train,X_val,y_val,classes,2,\"cat-dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task_4(\"cat-dog\",X_train,y_train,X_val,y_val,classes,[['relu', 'swish', 'swish'],[512, 128, 128]],words=None,maxEpochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swish Function to Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://stackoverflow.com/questions/43915482/how-do-you-create-a-custom-activation-function-with-keras\n",
    "from keras.layers import Activation\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "def swish2(x):\n",
    "    return x*K.sigmoid(x)\n",
    "\n",
    "get_custom_objects().update({'swish': Activation(swish2)})\n",
    "\n",
    "def addswish(model):\n",
    "    model.add(Activation(swish2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task5(X_train,y_train,X_val,y_val,classes,HLList,ALList):\n",
    "    model = Sequential()\n",
    "    # Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "    # in the first layer, you must specify the expected input data shape:\n",
    "    # here, 20-dimensional vectors.\n",
    "    m=X_train.shape[1]\n",
    "    l=y_train.shape[1]\n",
    "    \n",
    "    model.add(Dense(HLList[0], activation=ALList[0], input_dim=m))\n",
    "    model.add(Dropout(0.5))\n",
    "    flag=True\n",
    "    for x,y in zip(HLList,ALList):\n",
    "        if(flag):\n",
    "            flag=False\n",
    "            continue\n",
    "        model.add(Dense(x, activation=y))\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "    model.add(Dense(l, activation='softmax'))\n",
    "\n",
    "    sgd = SGD(lr=0.01, decay=0, momentum=0.0, nesterov=False)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train,\n",
    "              epochs=20,\n",
    "              batch_size=128)\n",
    "    y_pred = model.predict(X_val)\n",
    "    #y_pred=oneHot(y_pred,classes)\n",
    "    #scoreTrain=model.evaluate(X_train, y_train, batch_size=128)\n",
    "    #scoreVal = model.evaluate(X_val, y_val, batch_size=128)\n",
    "    #print(scoreTrain,scoreVal)\n",
    "    y_val=np.argmax(y_val,axis=1)\n",
    "    y_pred=np.argmax(y_pred,axis=1)\n",
    "    #print(y_val,y_pred)\n",
    "    print(np.mean(y_val==y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bestParams,bestACC,bestHL\n",
    "#task5(X_train,y_train,X_val,y_val,classes,[512, 128, 128],['relu', 'swish', 'swish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('cat-dog_MODEL/full_res',[full,res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fullres=np.load('cat-dog_MODEL/full_res.npy').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataSetName=\"MNIST\"\n",
    "# fig_name='plots/{0}/{0}_{1}.png'.format(dataSetName,'RESULT')\n",
    "# plotGraph(task1res,fig_name,[dataSetName,'My_NN',1,1],plot=False)\n",
    "# #'DataSet={0}, model={1}, part={2}, task={3}'\n",
    "# #print('hello worlds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task1res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter\n",
    "# X_train,y_train,X_val,y_val,classes,words=Twitter()\n",
    "# X_train.shape,y_train.shape, X_val.shape, y_val.shape\n",
    "# net,X_val,y_val=getNET(X_train,y_train,X_val,y_val,classes,words=None)\n",
    "# costs=[]\n",
    "# costs=(net.train(initADAMS=True,batch_size=1000,doOp=True,epochs=15,KKK=1,earlyStopping=True,X_val=X_val,y_val=y_val,minEpochs=1,patience=0))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updates\n",
    "def Twitter():\n",
    "    X=np.array(LoadDataForTXT('D:/workspace/tipr/tipr 2nd ass/tipr-second-assignment/data/twitter/twitter.txt'))\n",
    "    y=np.array(LoadDataForTXT('D:/workspace/tipr/tipr 2nd ass/tipr-second-assignment/data/twitter/twitter_label.txt'))\n",
    "    y=y.T[0]\n",
    "    X,words=BagOfWords(X,keys=None)\n",
    "    X,y=shuffle(X,y)\n",
    "    X,y,classes=preprocess(X,y,\"Twitter\")\n",
    "    X_train,y_train,X_val,y_val=train_test_split(X,y,.2)\n",
    "    X_train=np.array(X_train)\n",
    "    y_train=np.array(y_train)\n",
    "    X_val=np.array(X_val)\n",
    "    y_val=np.array(y_val)\n",
    "    return X_train,y_train,X_val,y_val,classes,words\n",
    "# def getNET(X_train,y_train,X_val,y_val,classes,words):\n",
    "#     gm1=X_train.shape[1]*2\n",
    "#     gm2=round((gm1*y_train.shape[1])**(0.5))\n",
    "#     gm3=round((gm2*y_train.shape[1])**(0.5)) \n",
    "#     myList=np.array([[gm1,'relu',0.1],[gm2,'tanh',0.01]])\n",
    "#     net=neuralNetwork(X_train,y_train,classes,dataSetName=\"Twitter\",hiddenlayers=[gm1,gm2],activations=['relu','swish','soft-max'],cost='cross_entropy',learningRate=[.3,.03,0.0003])#mnist\n",
    "#     net.layers\n",
    "#     net.classes\n",
    "#     net.words=words\n",
    "#     return net,X_val,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummyDataSet(dataname):\n",
    "    X,y=sklearn.datasets.load_digits(n_class=10, return_X_y=True)\n",
    "    from sklearn.utils import shuffle\n",
    "    X,y=shuffle(X,y,random_state=26)\n",
    "\n",
    "    X,y,classes=preprocess(X,y,dataname)\n",
    "    X_train,y_train,X_val,y_val=train_test_split(X,y)\n",
    "    X_train=np.array(X_train)\n",
    "    y_train=np.array(y_train)\n",
    "    X_val=np.array(X_val)\n",
    "    y_val=np.array(y_val)\n",
    "    #return X_train,y_train,X_val,y_val\n",
    "\n",
    "    #X,y=sklearn.datasets.load_iris(return_X_y=True)\n",
    "    gm1=X.shape[1]*2\n",
    "    gm2=int((gm1*10)**(0.5))\n",
    "#X_train=np.copy(X)\n",
    "#y_train=np.copy(y)\n",
    "#classes=[i for i in range(0,10)]\n",
    "    print(X_train.shape)\n",
    "    net=neuralNetwork(X_train,y_train,classes,dataSetName=\"MNIST\",hiddenlayers=[gm1,gm2],activations=['relu','tanh','soft-max'],cost='L2',learningRate=[.1,.001,.0001])#mnist\n",
    "    #net=neuralNetwork(X,y.reshape(-1,1),hiddenlayers=[gm1],activations=['tanh','soft-max'],cost='L2',learningRate=[.1,.001])#iris\n",
    "\n",
    "    #net.y,net.classes=net.oneHot(y)\n",
    "    \n",
    "    net.layers\n",
    "    net.classes\n",
    "    #costs=net.train(epochs=1000)\n",
    "    #costs=net.train(batch_size=2,epochs=100,KKK=10,earlyStopping=True,X_val=X_val,y_val=y_val)  \n",
    "    #fig_name='myPlot2.png'\n",
    "    #plotGraph(net,costs,fig_name)\n",
    "    return net,X_val,y_val\n",
    "    net,X_val,y_val=dummyDataSet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catdog(model_path,path='D:/workspace/tipr/tipr 2nd ass/tipr-second-assignment/data/MNIST',IMG_SIZE=28):\n",
    "    X=np.array(loadDataSet2('{0}/{1}'.format(path,\"cat\") ,itr=None,IMG_SIZE=IMG_SIZE,as_gray=True))\n",
    "    y=[0]*X.shape[0]\n",
    "    \n",
    "    X=X.reshape(X.shape[0],-1)\n",
    "    print('cat',X.shape)\n",
    "    #X=scale(X)\n",
    "    for i in range(1,2):\n",
    "        tmp_X=np.array(loadDataSet2('{0}/{1}'.format(path,\"dog\"),itr=None,IMG_SIZE=IMG_SIZE,as_gray=True))\n",
    "        tmp_y=[i]*tmp_X.shape[0]\n",
    "        print(tmp_X.shape)\n",
    "        tmp_X=tmp_X.reshape(tmp_X.shape[0],-1)\n",
    "        #tmp_X=scale(tmp_X)\n",
    "       \n",
    "        X=np.append(X,tmp_X,axis=0)\n",
    "        y=np.append(y,tmp_y)\n",
    "        print(X.shape,len(y))\n",
    "    X,y=shuffle(X,y)\n",
    "    X,y,classes=preprocess(X,y,\"cat-dog\",model_path,doScale=True)\n",
    "    X_train,y_train,X_val,y_val=train_test_split(X,y,.2)\n",
    "    X_train=np.array(X_train)\n",
    "    y_train=np.array(y_train)\n",
    "    X_val=np.array(X_val)\n",
    "    y_val=np.array(y_val)\n",
    "    return X_train,y_train,X_val,y_val,classes\n",
    "# def getNET(X_train,y_train,X_val,y_val,classes):\n",
    "#     gm1=X_train.shape[1]//2\n",
    "#     gm2=round((gm1*y_train.shape[1])**(0.5))*4\n",
    "#     gm3=round((gm2*y_train.shape[1])**(0.5)) \n",
    "#     myList=np.array([[gm1,'relu',0.1],[gm2,'tanh',0.01]])\n",
    "#     net=neuralNetwork(X_train,y_train,classes,dataSetName=\"CAT_DOG\",hiddenlayers=[gm1,gm2],activations=['relu','tanh','soft-max'],cost='cross_entropy',learningRate=[.3,.01,.0001])#mnist\n",
    "#     net.layers\n",
    "#     net.classes\n",
    "#     return net,X_val,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updates\n",
    "def Dolphins():\n",
    "    X=np.array(LoadDataForCSV('D:/workspace/tipr/tipr 2nd ass/tipr-second-assignment/data/dolphins/dolphins.csv'))\n",
    "    y=np.array(LoadDataForCSV('D:/workspace/tipr/tipr 2nd ass/tipr-second-assignment/data/dolphins/dolphins_label.csv'))\n",
    "    y=y.T[0]\n",
    "    X,y=shuffle(X,y)\n",
    "    X,y,classes=preprocess(X,y,\"Dolphins\")\n",
    "    X_train,y_train,X_val,y_val=train_test_split(X,y,.2)\n",
    "    X_train=np.array(X_train)\n",
    "    y_train=np.array(y_train)\n",
    "    X_val=np.array(X_val)\n",
    "    y_val=np.array(y_val)\n",
    "    return X_train,y_train,X_val,y_val,classes\n",
    "# def getNET(X_train,y_train,X_val,y_val,classes):\n",
    "#     gm1=X_train.shape[1]*2\n",
    "#     gm2=round((gm1*y_train.shape[1])**(0.5))\n",
    "#     gm3=round((gm2*y_train.shape[1])**(0.5)) \n",
    "#     myList=np.array([[gm1,'relu',0.1],[gm2,'tanh',0.01]])\n",
    "#     net=neuralNetwork(X_train,y_train,classes,dataSetName=\"dolphins\",hiddenlayers=[gm1,gm2],activations=['tanh','tanh','soft-max'],cost='cross_entropy',learningRate=[.1,.1,0.1])#mnist\n",
    "#     net.layers\n",
    "#     net.classes\n",
    "#     return net,X_val,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updates\n",
    "def Pubmed():\n",
    "    X=np.array(LoadDataForCSV('tipr-second-assignment/data/pubmed/pubmed.csv'))\n",
    "    y=np.array(LoadDataForCSV('tipr-second-assignment/data/pubmed/pubmed_label.csv'))\n",
    "    y=y.T[0]\n",
    "    X,y=shuffle(X,y)\n",
    "    X,y,classes=preprocess(X,y,\"Pubmed\")\n",
    "    X_train,y_train,X_val,y_val=train_test_split(X,y,.2)\n",
    "    X_train=np.array(X_train)\n",
    "    y_train=np.array(y_train)\n",
    "    X_val=np.array(X_val)\n",
    "    y_val=np.array(y_val)\n",
    "    return X_train,y_train,X_val,y_val,classes\n",
    "# def getNET(X_train,y_train,X_val,y_val,classes):\n",
    "#     gm1=X_train.shape[1]*2\n",
    "#     gm2=round((gm1*y_train.shape[1])**(0.5))\n",
    "#     gm3=round((gm2*y_train.shape[1])**(0.5)) \n",
    "#     myList=np.array([[gm1,'relu',0.1],[gm2,'tanh',0.01]])\n",
    "#     net=neuralNetwork(X_train,y_train,classes,dataSetName=\"pubmed\",hiddenlayers=[gm1,gm2],activations=['relu','tanh','soft-max'],cost='cross_entropy',learningRate=[.1,.01,0.01])#mnist\n",
    "#     net.layers\n",
    "#     net.classes\n",
    "#     return net,X_val,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNIST(model_path,path='D:/workspace/tipr/tipr 2nd ass/tipr-second-assignment/data/MNIST'):\n",
    "    X=np.array(loadDataSet('{0}/{1}'.format(path,0)))\n",
    "    y=[0]*X.shape[0]\n",
    "    for i in range(1,10):\n",
    "        tmp_X=np.array(loadDataSet('{0}/{1}'.format(path,i)))\n",
    "        tmp_y=[i]*tmp_X.shape[0]\n",
    "        print(tmp_X.shape)\n",
    "        X=np.append(X,tmp_X,axis=0)\n",
    "        y=np.append(y,tmp_y)\n",
    "        print(X.shape,len(y))\n",
    "    X=X.reshape(X.shape[0],-1)\n",
    "    X,y=shuffle(X,y)\n",
    "    X,y,classes=preprocess(X,y,\"MNIST\",model_path,doScale=True)\n",
    "    X_train,y_train,X_val,y_val=train_test_split(X,y,.2)\n",
    "    X_train=np.array(X_train)\n",
    "    y_train=np.array(y_train)\n",
    "    X_val=np.array(X_val)\n",
    "    y_val=np.array(y_val)\n",
    "    return X_train,y_train,X_val,y_val,classes\n",
    "# def getNET(X_train,y_train,X_val,y_val,classes):\n",
    "#     gm1=X_train.shape[1]*2\n",
    "#     gm2=round((gm1*y_train.shape[1])**(0.5))\n",
    "#     gm3=round((gm2*y_train.shape[1])**(0.5)) \n",
    "#     myList=np.array([[gm1,'relu',0.1],[gm2,'tanh',0.01]])\n",
    "#     net=neuralNetwork(X_train,y_train,classes,dataSetName=\"MNIST\",hiddenlayers=[gm1,gm2],activations=['tanh','sigmoid','soft-max'],cost='L2',learningRate=[0.3,0.003])#mnist\n",
    "#     net.layers\n",
    "#     net.classes\n",
    "#     return net,X_val,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestMNIST(test_path,model_path=\"MNIST_MODEL/Model_Main.npy\"):\n",
    "        X=np.array(loadDataSet('{0}/{1}'.format(test_path,0)))\n",
    "        y=[0]*X.shape[0]\n",
    "        for i in range(1,10):\n",
    "            tmp_X=np.array(loadDataSet('{0}/{1}'.format(test_path,i)))\n",
    "            tmp_y=[i]*tmp_X.shape[0]\n",
    "            print(tmp_X.shape)\n",
    "            X=np.append(X,tmp_X,axis=0)\n",
    "            y=np.append(y,tmp_y)\n",
    "            print(X.shape,len(y))\n",
    "        X=X.reshape(X.shape[0],-1)\n",
    "        net,X,y=getNET(X,y,X,y,\"MNIST\",[1568,128],['relu','tanh'],classes)\n",
    "        net.loadModel(model_path)\n",
    "        X,y=preprocess(X,y,\"MNIST\",doScale=True,testing=True,net.classes)\n",
    "       \n",
    "        net.testModel(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tstpath='D:/workspace/tipr/tipr 2nd ass/tipr-second-assignment/data/MNIST'\n",
    "#TestMNIST(tstpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Testcatdog(test_path,model_path=\"cat-dog_MODEL/Model_Main.npy\",IMG_SIZE=28):\n",
    "        X=np.array(loadDataSet2('{0}/{1}'.format(test_path,\"cat\") ,itr=None,IMG_SIZE=IMG_SIZE,as_gray=True))\n",
    "        y=[0]*X.shape[0]\n",
    "\n",
    "        X=X.reshape(X.shape[0],-1)\n",
    "        print('cat',X.shape)\n",
    "        #X=scale(X)\n",
    "        for i in range(1,2):\n",
    "            tmp_X=np.array(loadDataSet2('{0}/{1}'.format(test_path,\"dog\"),itr=None,IMG_SIZE=IMG_SIZE,as_gray=True))\n",
    "            tmp_y=[i]*tmp_X.shape[0]\n",
    "            print(tmp_X.shape)\n",
    "            tmp_X=tmp_X.reshape(tmp_X.shape[0],-1)\n",
    "            #tmp_X=scale(tmp_X)\n",
    "\n",
    "            X=np.append(X,tmp_X,axis=0)\n",
    "            y=np.append(y,tmp_y)\n",
    "            print(X.shape,len(y))\n",
    "            \n",
    "        net,X,y=getNET(X,y,X,y,\"cat-dog\",[1568,128],['relu','tanh'],classes)\n",
    "        net.loadModel(model_path)\n",
    "        X,y=preprocess(X,y,\"cat-dog\",doScale=True,testing=True,net.classes)\n",
    "        net.testModel(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tstpath='D:/workspace/tipr/tipr 2nd ass/tipr-second-assignment/data/cat-dog'\n",
    "#Testcatdog(tstpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNET(X_train,y_train,X_val,y_val,dataSetName=\"Twitter\",HLList=[1568,128],activations=['relu','tanh'],classes=None,words=None):\n",
    "    alphas=[10**(-i) for i in range(1,len(HLList)+1)]\n",
    "    lr=alphas \n",
    "    net=neuralNetwork(X_train,y_train,classes,dataSetName=dataSetName,hiddenlayers=HLList,activations=activations+['soft-max'],cost='L2',learningRate=lr+[10**(-(len(HLList)+1)) ])#mnist\n",
    "    net.layers\n",
    "    net.classes\n",
    "    net.words=words\n",
    "    return net,X_val,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net,X_val,y_val=getNET(X_train,y_train,X_val,y_val,\"cat-dog\",[512, 128, 128],['relu', 'swish', 'swish'],classes)\n",
    "#costs=(net.train(initADAMS=False,batch_size=1000,doOp=False,epochs=20,KKK=1,earlyStopping=True,X_val=X_val,y_val=y_val,printResults=True,minEpochs=1,patience=0))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net.learningRate=np.array([0.03]*2)\n",
    "#net.testModel(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net.learningRate[0]=0.3\n",
    "#net.learningRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#costs=(net.train(initADAMS=False,batch_size=1000,doOp=False,epochs=20,KKK=1,earlyStopping=True,X_val=X_val,y_val=y_val,printResults=True,minEpochs=1,patience=0))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net.loadModel(\"cat-dog_MODEL/Model_Main.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net.loadModel(\"xxx.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.dataSetName=\"cat-dog\"\n",
    "# fig_name='plots/{0}/{0}_{1}.png'.format(net.dataSetName,'9')\n",
    "# plotGraph(costs,fig_name,net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[0].reshape(100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.imshow('image',X_train[0].reshape(100,100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [np.array(i).shape for i in net.bias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [np.min(i) for i in net.bias[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far For Mnist Dataset.\n",
    "Config is:-<br>\n",
    "<b>net=neuralNetwork(X,y.reshape(-1,1),hiddenlayers=[128,gm],activations=['relu','tanh','soft-max'],cost='L2',learningRate=[.01,.01,.01])</b>  acc = 99.99% <br><br>\n",
    "net=neuralNetwork(X,y.reshape(-1,1),hiddenlayers=[128,gm],activations=['relu','relu','soft-max'],cost='L2',learningRate=[.01,.01,.01]) acc= 99% but chokes for NAN<br>\n",
    "<br>For Scale :-\n",
    "Normalize is set to false for both case. Min-max works better. with norm then minmax, cost goes down but slower than just minmax.\n",
    "<br>Seed set to 26.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks to be done.\n",
    "1- Weight Init methods\n",
    "2- Random/uniform method.\n",
    "3- grid search.--store F1 score, accuracy for no. of layer count\n",
    "4-swish Implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
