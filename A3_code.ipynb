{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Mohit Code.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "4nojBgxUSIsq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "import datetime\n",
        "import os\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "import matplotlib as mpl\n",
        "#mpl.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle, gzip\n",
        "import sys\n",
        "# import sys,shutil,datetime,pickle,codecs,tempfile, gzip\n",
        "\n",
        "# import numpy as np\n",
        "# import scipy as sp\n",
        "import pandas as pd\n",
        "# import matplotlib as mpl\n",
        "# import cv2\n",
        "\n",
        "# import sklearn\n",
        "# import skimage\n",
        "\n",
        "# import tensorflow as tf\n",
        "# import keras\n",
        "from sklearn.cluster import KMeans, MiniBatchKMeans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v8rGViOGmZ5z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def scale(X,testing=False,mode='standard',a=None,b=None):\n",
        "    #X=np.nan_to_num\n",
        "    X=np.array(X)\n",
        "    if  mode=='scale':\n",
        "        if(not testing):\n",
        "            mx,mn=X.max(axis=0),X.min(axis=0)\n",
        "        else:\n",
        "            mx,mn=b,a\n",
        "        mx=np.where(mx==mn,mx+1,mx)\n",
        "        X=(X-mn)/(mx-mn)\n",
        "        if(testing):return X\n",
        "        return X,mn,mx\n",
        "    elif mode=='standard':\n",
        "        if(not testing):\n",
        "            mean,std=X.mean(axis=0),X.std(axis=0)\n",
        "        else:\n",
        "            mean,std=a,b\n",
        "        std=np.where(std==0,1,std)\n",
        "        X=(X-mean)/std\n",
        "        if(testing):return X\n",
        "        return X,mean,std\n",
        "\n",
        "def preprocess(X,model_name,mode='standard',doScale=True,testing=False):\n",
        "        if(doScale):\n",
        "            if(not testing):\n",
        "                X,a,b=scale(X,testing,mode=mode)\n",
        "                \n",
        "                if not os.path.isdir(model_name+\"_MODEL\"):\n",
        "                   os.makedirs(model_name+\"_MODEL\")\n",
        "                np.save('{0}_MODEL/A'.format(model_name),a)\n",
        "                np.save('{0}_MODEL/B'.format(model_name),b)\n",
        "                return X\n",
        "            else:\n",
        "                a=np.load('{0}_MODEL/A.npy'.format(model_name)).tolist()\n",
        "                b=np.load('{0}_MODEL/B.npy'.format(model_name)).tolist()\n",
        "                X=scale(X,testing,'standard',a,b)\n",
        "                return X\n",
        "\n",
        "def unpickle(file,is_bytes=True):\n",
        "  with open(file, 'rb') as fp:\n",
        "    dict = pickle.load(fp, encoding='bytes')\n",
        "  return dict\n",
        "\n",
        "def load_mnist(path,kind):\n",
        "  'Load MNIST data from `path'\n",
        "  labels_path = os.path.join(path, '%s-labels-idx1-ubyte.gz' % kind)\n",
        "  images_path = os.path.join(path, '%s-images-idx3-ubyte.gz' % kind)\n",
        "\n",
        "  with gzip.open(labels_path, 'rb') as lbpath:\n",
        "    labels = np.frombuffer(lbpath.read(), dtype=np.uint8, offset=8)\n",
        "\n",
        "  with gzip.open(images_path, 'rb') as imgpath:\n",
        "    images = np.frombuffer(imgpath.read(), dtype=np.uint8, offset=16).reshape(len(labels), 784)\n",
        "  images=pd.DataFrame(images)\n",
        "  images = np.array( [np.array(x).reshape((28,28,1)) for i,x in images.iterrows()] )\n",
        "  return images, labels\n",
        "def load_cifar(data_path = 'data/CIFAR-10'):\n",
        "  print('path',data_path)\n",
        "  X,Y,target_names = None, None, None\n",
        "  for file_name in os.listdir(data_path):\n",
        "    if 'data_batch' in file_name:\n",
        "      temp = unpickle( os.path.join(data_path,file_name) )\n",
        "      X = pd.concat( (pd.DataFrame(temp[b'data']),  X) ) if X is not None else pd.DataFrame( temp[b'data']   )\n",
        "      Y = pd.concat( (pd.DataFrame(temp[b'labels']),Y) ) if Y is not None else pd.DataFrame( temp[b'labels'] )\n",
        "    elif 'batches' in file_name:\n",
        "      temp = unpickle( '/'.join((data_path,file_name)) )\n",
        "      target_names = temp[b'label_names']\n",
        "  X = np.array( [np.array(x).reshape((32,32,3)) for i,x in X.iterrows()] )\n",
        "\n",
        "  #X.to_csv( '/'.join((data_path,'X.csv')), header=False,index=False )\n",
        "  #Y.to_csv( '/'.join((data_path,'Y.csv')), header=False,index=False )\n",
        "\n",
        "  #datasets['CIFAR-10'] = (X,Y,[x.decode('ascii') for x in target_names])\n",
        "  return X,Y\n",
        "\n",
        "def train_test_split(X,Y,size=0.2):\n",
        "  #Cross-validation -- to be done via k-fold later.\n",
        "  from sklearn.model_selection import train_test_split  \n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=size)\n",
        "  return X_train, Y_train,X_test, Y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XnujxOFreHV5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# batch_size = 32\n",
        "# num_classes = 10\n",
        "# epochs = 5\n",
        "# save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "# model_name = 'Fashion-MNIST'\n",
        "# data_path=os.path.join(os.getcwd(), 'tipr-third-assignment\\data')\n",
        "# if model_name=='CIFAR-10':\n",
        "#   print('working on CIFAR-10')\n",
        "#   X,y=load_cifar(data_path+'\\\\'+model_name)\n",
        "#   print('x_train shape:', X.shape)\n",
        "#   print(X.shape[0], 'train samples') \n",
        "#   X=preprocess(X,model_name,mode='standard',doScale=True,testing=False)\n",
        "#   y = tensorflow.keras.utils.to_categorical(y, num_classes)\n",
        "#   x_train, y_train,x_test, y_test=train_test_split(X,y,size=0.2)\n",
        "# elif model_name=='Fashion-MNIST':\n",
        "#   print('working on Fashion-MNIST')\n",
        "#   X,y=load_mnist(data_path+'\\\\'+model_name,'train')\n",
        "#   print('x_train shape:', X.shape)\n",
        "#   print(X.shape[0], 'train samples') \n",
        "#   X=preprocess(X,model_name,mode='standard',doScale=True,testing=False)\n",
        "#   y = tensorflow.keras.utils.to_categorical(y, num_classes)\n",
        "#   x_train, y_train,x_test, y_test=train_test_split(X,y,size=0.2)\n",
        "\n",
        "# # # The data, split between train and test sets:\n",
        "# # (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "# # print('x_train shape:', x_train.shape)\n",
        "# # print(x_train.shape[0], 'train samples')\n",
        "# # print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# # # Convert class vectors to binary class matrices.\n",
        "# # y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\n",
        "# # y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)\n",
        "# # #x_train,x_test=scale_0_1(x_train,x_test)\n",
        "# # x_train=preprocess(x_train,model_name,mode='standard',doScale=True,testing=False)\n",
        "# # x_test=preprocess(x_test,model_name,mode='standard',doScale=True,testing=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gmgbqMsI8Z6F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #####\n",
        "# if os.path.isdir(save_dir) and model_name in os.listdir(save_dir):\n",
        "#     model_path = os.path.join(save_dir, model_name)\n",
        "#     model2=tensorflow.keras.models.load_model(model_path)\n",
        "#     print('Loading Saved Model')\n",
        "# else:\n",
        "  \n",
        "#   layers=[512,200]\n",
        "#   acts=['relu']*len(layers)\n",
        "#   filters=[4,4]\n",
        "#   config=[layers]+[acts]+[filters]\n",
        "#   print('Config:',config)\n",
        "#   model2=CreateModel(x_train,num_classes,config)\n",
        "#   print('Creating New One')\n",
        "# #model=alexnet(x_train,num_classes)\n",
        "\n",
        "\n",
        "# trainModel(model2,save_dir, model_name,x_train,y_train,x_test,y_test,batch_size,epochs,tbc=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aYccxlygWKps",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainModel(model,save_dir, model_name,x_train,y_train,x_test,y_test,batch_size,epochs,tbc,issaveModel=True):\n",
        "  \n",
        "  model.fit(x_train, y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(x_test, y_test),\n",
        "            shuffle=True,\n",
        "            callbacks=[])\n",
        "\n",
        "  # Save model and weights\n",
        "  if not os.path.isdir(save_dir):\n",
        "      os.makedirs(save_dir)\n",
        "  model_path = os.path.join(save_dir, model_name)\n",
        "  \n",
        "  if model_name in os.listdir(save_dir):\n",
        "    print('Creating New Instance')\n",
        "    os.remove(model_path)\n",
        "  if issaveModel:\n",
        "    model.save(model_path)\n",
        "    print('Saved trained model at %s ' % model_path)\n",
        "\n",
        "  # Score trained model.\n",
        "  scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "  print('Test loss:', scores[0])\n",
        "  print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "11140wMFRQhH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def CreateModel(x_train,num_classes, config,mode='glorot_uniform'):\n",
        "  model = Sequential()\n",
        "  actor=config[1][0]\n",
        "  model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                   input_shape=x_train.shape[1:]))\n",
        "  model.add(Activation(actor))\n",
        "  \n",
        "  #filter\n",
        "  filters=config[2]\n",
        "  for filter in filters:\n",
        "    model.add(Conv2D(32, (filter, filter), padding='same'))\n",
        "    model.add(Activation(actor))            \n",
        "    #Pooling\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "  \n",
        "\n",
        "  model.add(Flatten())\n",
        "  #DENSE-FULLY CONNECTED LAYERS\n",
        "  for layer,act in zip(config[0],config[1]):\n",
        "    model.add(Dense(layer,kernel_initializer=mode))\n",
        "    model.add(Activation(act))\n",
        "    model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes,kernel_initializer=mode))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  # initiate RMSprop optimizer\n",
        "  opt = tensorflow.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
        "\n",
        "  # Let's train the model using RMSprop\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=opt,\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AIny4wMZ_UFm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def getF1Scores(aa,bb):\n",
        "        micron=sklearn.metrics.f1_score(aa,bb,average='micro')\n",
        "        macron=sklearn.metrics.f1_score(aa,bb,average='macro')\n",
        "        return micron, macron"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CgDdFbMcSblx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# layers=[512,200]\n",
        "# acts=['relu']*len(layers)\n",
        "# filters=[4,4]\n",
        "# config=[layers]+[acts]+[filters]\n",
        "\n",
        "# model=CreateModel(x_train,num_classes,config)   \n",
        "# trainModel(model,x_train,y_train,x_test,y_test,batch_size,epochs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HyshhyL6XM67",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def scale_0_1(x_train,x_test):\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_test = x_test.astype('float32')\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "  return x_train,x_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8BcQKbWz-dKz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# %tensorboard --logdir=/content/log/fit\n",
        "# #!rm -rf ./log/\n",
        "# log_dir=\"log/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "# tbc = tensorflow.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "# #!pip uninstall -q tf-nightly-2.0-preview\n",
        "\n",
        "# # Load the TensorBoard notebook extension\n",
        "# from tensorboard import *\n",
        "# %load_ext tensorboard.notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C3cuFaTCmhMy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def task1(x_train,y_train,x_test,y_test):  \n",
        "  batch_size = 32\n",
        "  num_classes = 10\n",
        "  epochs = 20\n",
        "  save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "  model_name = 'Task1'\n",
        "  \n",
        "  alayers=[512,128,50]\n",
        "  total={}\n",
        "  for i in range(len(alayers)):\n",
        "    layers=alayers[0:(i+1)]\n",
        "    acts=['relu']*len(layers)\n",
        "    filters=[5]*len(layers)\n",
        "    config=[layers]+[acts]+[filters]\n",
        "    print('Config:',config)\n",
        "    model=CreateModel(x_train,num_classes,config)\n",
        "    print('Creating New One')\n",
        "    #Hope data is scaled...\n",
        "    trainModel(model,save_dir, model_name+'layers'+str(i),x_train,y_train,x_test,y_test,batch_size,epochs,tbc=None)\n",
        "    y_pred=model.predict(x_train)\n",
        "    y_pred1=1*(y_pred==y_pred.max(axis=1,keepdims=True))\n",
        "    score=model.evaluate(x_train, y_train, verbose=1)\n",
        "    y_pred=model.predict(x_test)\n",
        "    y_pred=1*(y_pred==y_pred.max(axis=1,keepdims=True))\n",
        "    score=score+[sklearn.metrics.accuracy_score(y_test,y_pred)]+list(getF1Scores(y_train,y_pred1))\n",
        "    total[i+1]=(score)\n",
        "    print(score)\n",
        "  plotGraph(total,model_name,title=model_name)\n",
        "  return total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rbpJww20JVB1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# task1(x_train,y_train,x_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1z7EdrEcDhdB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plotGraph(costs,fig_name,Xtitle='Layer Count',title=\"Some Graph\"):\n",
        "    aa=list(costs.values())   \n",
        "    aa=np.array([list(i) for i in  aa])\n",
        "    a1,a2,a3,a4,a5=aa.T #accuracy, cost\n",
        "   \n",
        "    plt.figure(num=None, figsize=(4, 4), dpi=300, facecolor='w', edgecolor='k')\n",
        "    #write after this line.\n",
        "    plt.ylabel(\"Accuracy/Cost<Scaled-down by max={0}>\".format(int(np.max(a1))))\n",
        "    plt.xlabel(Xtitle)\n",
        "    plt.title(title)\n",
        "    plt.subplot().plot(list(costs.keys()),a2,'*',label='Accuracy on Train Set')\n",
        "    plt.subplot().plot(list(costs.keys()),a1/np.max(a1),'b', label='Cost of Train Data')\n",
        "    plt.subplot().plot(list(costs.keys()),a3,'b--', label='Accuracy on Validation Set')\n",
        "    plt.subplot().plot(list(costs.keys()),a4, label='f1-micro')\n",
        "    plt.subplot().plot(list(costs.keys()),a5, label='f1-macro')\n",
        "    \n",
        "    plt.legend(loc='best', shadow=False)\n",
        "\n",
        "    plt.savefig(fig_name)\n",
        "    plt.show()\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MAGJRwPsGfHk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def task2(x_train,y_train,x_test,y_test):  \n",
        "  batch_size = 1024\n",
        "  num_classes = 10\n",
        "  epochs = 10\n",
        "  save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "  model_name = 'Task2'\n",
        "  \n",
        "  alayers=[128,128,128]\n",
        "  for i in range(len(alayers)):\n",
        "    layers=alayers[0:i]\n",
        "    acts=['relu']*(len(layers)+1)\n",
        "    filters=[5]*(len(layers)+1)\n",
        "    val=1\n",
        "    total={}\n",
        "    while(val<alayers[i]):\n",
        "      config=[layers+[val]]+[acts]+[filters]\n",
        "      print('Config:',config)\n",
        "      model=CreateModel(x_train,num_classes,config)\n",
        "      print('Creating New One')\n",
        "      #Hope data is scaled...\n",
        "      trainModel(model,save_dir, model_name+'layers'+str(i)+'_val_'+str(val),x_train,y_train,x_test,y_test,batch_size,epochs,tbc=None)\n",
        "      y_pred=model.predict(x_train)\n",
        "      y_pred1=1*(y_pred==y_pred.max(axis=1,keepdims=True))\n",
        "      score=model.evaluate(x_train, y_train, verbose=1)\n",
        "      y_pred=model.predict(x_test)\n",
        "      y_pred=1*(y_pred==y_pred.max(axis=1,keepdims=True))\n",
        "      score=score+[sklearn.metrics.accuracy_score(y_test,y_pred)]+list(getF1Scores(y_train,y_pred1))\n",
        "      total[val]=(score)\n",
        "      print(score)\n",
        "      val=val*2\n",
        "    print(\"total::::\",total)\n",
        "    plotGraph(total,model_name+'layers'+str(i+1),model_name+'layers'+str(i))\n",
        "# task2(x_train,y_train,x_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rVULwcytCJ8L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def task3(x_train,y_train,x_test,y_test):  \n",
        "  batch_size = 512\n",
        "  num_classes = 10\n",
        "  epochs = 10\n",
        "  save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "  model_name = 'Task3'\n",
        "  \n",
        "  alayers=[512,128,64]\n",
        "  actors=['relu','swish','tanh','sigmoid']\n",
        "  total={}\n",
        "  for actor in actors:\n",
        "    layers=alayers\n",
        "    acts=[actor]*(len(layers))\n",
        "    filters=[5]*(len(layers))\n",
        "    \n",
        "    config=[alayers]+[acts]+[filters]\n",
        "    print('Config:',config)\n",
        "    model=CreateModel(x_train,num_classes,config)\n",
        "    print('Creating New One')\n",
        "    #Hope data is scaled...\n",
        "    trainModel(model,save_dir, model_name+actor,x_train,y_train,x_test,y_test,batch_size,epochs,tbc=None,issaveModel=False)\n",
        "    y_pred=model.predict(x_train)\n",
        "    y_pred1=1*(y_pred==y_pred.max(axis=1,keepdims=True))\n",
        "    score=model.evaluate(x_train, y_train, verbose=1)\n",
        "    y_pred=model.predict(x_test)\n",
        "    y_pred=1*(y_pred==y_pred.max(axis=1,keepdims=True))\n",
        "    score=score+[sklearn.metrics.accuracy_score(y_test,y_pred)]+list(getF1Scores(y_train,y_pred1))\n",
        "    total[actor]=(score)\n",
        "    print(score)\n",
        "  plotGraph(total,model_name,'Activations',model_name)\n",
        "# task3(x_train,y_train,x_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dnN-tmRoCgiZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import get_custom_objects\n",
        "\n",
        "def swish2(x):\n",
        "    return x*K.sigmoid(x)\n",
        "\n",
        "get_custom_objects().update({'swish': Activation(swish2)})\n",
        "\n",
        "def addswish(model):\n",
        "    model.add(Activation(swish2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dsMv4EEeES44",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def task4(x_train,y_train,x_test,y_test):\n",
        "  batch_size = 512\n",
        "  num_classes = 10\n",
        "  epochs = 10\n",
        "  save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "  \n",
        "  model_name='Task4'\n",
        "  layers=[512,200]\n",
        "  acts=['relu']*len(layers)\n",
        "  filters=[4,4]\n",
        "  \n",
        "  modes=['glorot_uniform','glorot_normal']\n",
        "  total={}\n",
        "  for mode in modes:\n",
        "    config=[layers]+[acts]+[filters]\n",
        "    print('Config:',config)\n",
        "    model=CreateModel(x_train,num_classes,config,mode)\n",
        "    print('Creating New One')\n",
        "    \n",
        "    trainModel(model,save_dir, model_name,x_train,y_train,x_test,y_test,batch_size,epochs,tbc=None,issaveModel=False)\n",
        "    y_pred=model.predict(x_train)\n",
        "    y_pred1=1*(y_pred==y_pred.max(axis=1,keepdims=True))\n",
        "    score=model.evaluate(x_train, y_train, verbose=1)\n",
        "    y_pred=model.predict(x_test)\n",
        "    y_pred=1*(y_pred==y_pred.max(axis=1,keepdims=True))\n",
        "    score=score+[sklearn.metrics.accuracy_score(y_test,y_pred)]+list(getF1Scores(y_train,y_pred1))\n",
        "    total[mode]=(score)\n",
        "    print(score)\n",
        "  \n",
        "  \n",
        "  plotGraph(total,model_name,'Weight Inits',model_name)\n",
        "  \n",
        "  \n",
        "# task4(x_train,y_train,x_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I-vVbHP09DoP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def task5_6(X,y):\n",
        "  \n",
        "  for i in range(1,6):\n",
        "    x_train, y_train,x_test, y_test=train_test_split(X,y,size=i *0.1)\n",
        "    layers=[128,32]\n",
        "    acts=[activation]*len(layers)\n",
        "    filters=config\n",
        "    config=[layers]+[acts]+[filters]\n",
        "    model=CreateModel(x_train,num_classes,config)   \n",
        "    trainModel(model,'saved_models', 'task5_'+str(i),x_train,y_train,x_test,y_test,batch_size,epochs,tbc=None)\n",
        "    lyr=vectorize(model)\n",
        "    embeds=get_embeds(model,x_test,lyr,batch_size=512)\n",
        "    clustering(embeds,list(range(10)))\n",
        "    taskTSNE(embeds,y)\n",
        "#task5_6(x_train,y_train)          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3NAH2u19s0m0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def task7(X_train,y_train,X_val,y_val):\n",
        "  \n",
        "  model = Sequential()\n",
        "  \n",
        "  m=X_train.shape[1]\n",
        "  l=y_train.shape[1]\n",
        "  layers=[512,200]\n",
        "  acts=['relu']*len(layers)\n",
        "  filters=[4,4]\n",
        "  config=[layers]+[acts]+[filters]\n",
        "  model.add(Flatten(input_shape=X_train.shape[1:]))\n",
        "  #model.add(Dense(512, activation=acts[0], input_dim=m))\n",
        "  #model.add(Dropout(0.5))\n",
        "  flag=True\n",
        "  for layer,act in zip(config[0],config[1]):  \n",
        "      model.add(Dense(layer, activation=act))\n",
        "      model.add(Dropout(0.5))\n",
        "\n",
        "  model.add(Dense(l, activation='softmax'))\n",
        "\n",
        "  opt = tensorflow.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
        "  # Let's train the model using RMSprop\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=opt,\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  model.fit(X_train, y_train,\n",
        "            epochs=500,\n",
        "            batch_size=1024)\n",
        "  y_pred = model.predict(X_val)\n",
        "  #y_pred=oneHot(y_pred,classes)\n",
        "  #scoreTrain=model.evaluate(X_train, y_train, batch_size=128)\n",
        "  #scoreVal = model.evaluate(X_val, y_val, batch_size=128)\n",
        "  #print(scoreTrain,scoreVal)\n",
        "  y_val=np.argmax(y_val,axis=1)\n",
        "  y_pred=np.argmax(y_pred,axis=1)\n",
        "  #print(y_val,y_pred)\n",
        "  print('Validation Accuracy:',np.mean(y_val==y_pred))\n",
        "#task7(x_train,y_train,x_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wpdkFoOeJuFs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 512\n",
        "num_classes = 10\n",
        "epochs = 5\n",
        "#argv=\"--test-data|D:\\\\workspace\\\\tipr\\\\tipr 3\\\\tipr-third-assignment\\\\data\\\\|--dataset|Fashion-MNIST\".split(\"|\")\n",
        "#argv=\"--train-data|D:\\\\workspace\\\\tipr\\\\tipr 3\\\\tipr-third-assignment\\\\data\\\\|--test-data|D:\\\\workspace\\\\tipr\\\\tipr 3\\\\tipr-third-assignment\\\\data\\\\|--dataset|CIFAR-10|--filter-config|[8|4|4]|--activation|relu\".split(\"|\")\n",
        "argv = list(sys.argv)\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "dataSetName = argv[argv.index('--dataset')+1]\n",
        "test_path = argv[argv.index('--test-data')+1]\n",
        "#test_path='D:\\\\workspace\\\\tipr\\\\tipr 3\\\\tipr-third-assignment\\\\data\\\\'\n",
        "model_name = dataSetName\n",
        "model_path=\"\"\n",
        "\n",
        "if '--train-data' in argv:\n",
        "    print('training Model')\n",
        "    train_path = argv[argv.index('--train-data')+1]\n",
        "    config   = []#HiddenLayers for now.\n",
        "    activation=argv[argv.index('--activation')+1]\n",
        "    for st in argv[argv.index('--filter-config')+1:]:\n",
        "        st  = st.strip()\n",
        "        if st.endswith(']'):\n",
        "            config.append(int(st.strip('[]')))\n",
        "            break\n",
        "        else:\n",
        "            config.append(int(st.strip('[]')))\n",
        "    data_path=train_path\n",
        "    if model_name=='CIFAR-10':\n",
        "      print('working on CIFAR-10')\n",
        "      X,y=load_cifar(os.path.join(data_path,model_name))\n",
        "      print('x_train shape:', X.shape)\n",
        "      print(X.shape[0], 'train samples') \n",
        "      X=preprocess(X,'temp_'+model_name,mode='standard',doScale=True,testing=False)\n",
        "      y = tensorflow.keras.utils.to_categorical(y, num_classes)\n",
        "      x_train, y_train,x_test, y_test=train_test_split(X,y,size=0.2)\n",
        "    elif model_name=='Fashion-MNIST':\n",
        "      print('working on Fashion-MNIST')\n",
        "      X,y=load_mnist(os.path.join(data_path,model_name),'train')\n",
        "      print('x_train shape:', X.shape)\n",
        "      print(X.shape[0], 'train samples') \n",
        "      X=preprocess(X,'temp_'+model_name,mode='standard',doScale=True,testing=False)\n",
        "      y = tensorflow.keras.utils.to_categorical(y, num_classes)\n",
        "      x_train, y_train,x_test, y_test=train_test_split(X,y,size=0.2)\n",
        "    \n",
        "    layers=[512,200]\n",
        "    acts=[activation]*len(layers)\n",
        "    filters=config\n",
        "    config=[layers]+[acts]+[filters]\n",
        "    model=CreateModel(x_train,num_classes,config)   \n",
        "    trainModel(model,save_dir, model_name,x_train,y_train,x_test,y_test,batch_size,epochs,tbc=None)\n",
        "\n",
        "  \n",
        "        \n",
        "else:\n",
        "    \n",
        "    model_path=os.path.join(os.getcwd(), 'saved_main_models')\n",
        "    if os.path.isdir(model_path) and model_name in os.listdir(model_path):\n",
        "        model=tensorflow.keras.models.load_model(os.path.join(model_path,model_name))\n",
        "        print('Loading Saved Model from',model_path)\n",
        "    else:\n",
        "        raise Exception('Model not found!!!',model_path)\n",
        "if(dataSetName==\"Fashion-MNIST\"):\n",
        "    x_test,y_test=load_mnist(os.path.join(test_path,model_name),'test')\n",
        "elif(dataSetName==\"CIFAR-10\" ):\n",
        "    x_test,y_test=load_cifar(os.path.join(test_path,model_name)) \n",
        "x_test=preprocess(x_test,model_name,mode='standard',doScale=True,testing=True)\n",
        "y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)\n",
        "  \n",
        "\n",
        "y_pred=model.predict(x_test)\n",
        "y_pred=1*(y_pred==y_pred.max(axis=1,keepdims=True))\n",
        "score=[sklearn.metrics.accuracy_score(y_test,y_pred)]+list(getF1Scores(y_test,y_pred))\n",
        "print('\\n\\nTest Accuracy::{0}\\nF1-micro::{1}\\nF1-macro::{2}'.format(score[0],score[1],score[2]))\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y36re6oXVOv0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def vectorize(model,batch_size = 100):\n",
        "  X, flg, layers_name = None, False, list(map(str,model.layers))[::-1]\n",
        "  for lyr in range(len(layers_name)):\n",
        "    if 'Activation' in layers_name[lyr]:\n",
        "      if flg:\n",
        "        break\n",
        "      flg = True\n",
        "  lyr = -1*(lyr+1)\n",
        "  print(model.layers)\n",
        "  return lyr\n",
        "def get_embeds(model,data,lyr,batch_size=100):\n",
        "  repr_fun = K.function([model.layers[0].input], [model.layers[lyr].output])\n",
        "  for i in range(int(np.ceil(len(data)/batch_size))):\n",
        "    tmp = repr_fun([data[i*batch_size:(i+1)*batch_size]])[0]\n",
        "    X = pd.concat( (X,pd.DataFrame(tmp)) ) if X is not None else pd.DataFrame(tmp)\n",
        "  vec_rep = np.array(X)\n",
        "  return vec_rep\n",
        "  \n",
        "def clustering(data,labels):\n",
        "  km = MiniBatchKMeans(n_clusters=10).fit(data)\n",
        "  y_true, y_n = np.argmax(labels,axis=1), km.labels_\n",
        "\n",
        "  l2i = {}\n",
        "  for i in range(len(data)):\n",
        "    if y_n[i] not in l2i:\n",
        "      l2i[ y_n[i] ] = {i}\n",
        "    else:\n",
        "      l2i[ y_n[i] ].add(i)\n",
        "\n",
        "  bestcluster = {}\n",
        "  for lbl in l2i:\n",
        "    tmp = {}\n",
        "    for i in l2i[lbl]:\n",
        "      if y_true[i] not in tmp:\n",
        "        tmp[y_true[i]] =  1\n",
        "      else:\n",
        "        tmp[y_true[i]] += 1\n",
        "    bestcluster[lbl] = tmp\n",
        "  remaining_labels = set(list(range(10)))\n",
        "  lbl2pred = {}\n",
        "  TRUE, FALSE = 0, 0\n",
        "  lambda2=lambda x: bestcluster[lbl][x] if (x in remaining_labels) else 0\n",
        "  for _ in range(10):\n",
        "    assgn_lbl = max( l2i,key=lambda lbl:max(bestcluster[lbl], key=lambda2(x)) )\n",
        "    pred_freq = sorted(bestcluster[assgn_lbl].items(),key=lambda x:x[1],reverse=True)\n",
        "    for act_lbl, true_freq in pred_freq:\n",
        "      if act_lbl in remaining_labels:\n",
        "        TRUE  += true_freq\n",
        "        FALSE += sum(y for x,y in pred_freq) - true_freq\n",
        "        lbl2pred[assgn_lbl] = [act_lbl]\n",
        "        break\n",
        "    else:\n",
        "      raise Exception('No remaing Label, should not happen')\n",
        "  print('Clustering Accuracy is {0:.2f}%'.format(100*TRUE/(TRUE+FALSE)))\n",
        "\n",
        "def taskTSNE(X,y):\n",
        "  target_names=[i for i in range(10)]\n",
        "  plt.legend(bbox_to_anchor=(1.08,0.7), loc=\"center left\", borderaxespad=0)\n",
        "  plt.axis('off')\n",
        "  if len(y.shape)>1:\n",
        "    y2 = np.argmax(y,axis=1)\n",
        "  else:\n",
        "    y2 = y\n",
        "  y2set = set(y2)\n",
        "  X2 = TSNE(n_iter=250).fit(X)\n",
        "  for y_n in y2_set:\n",
        "    x_n = X2[ [i for i in range(X2.shape[0]) if y2[i]==y_n] ].T\n",
        "    plt.scatter(x_n[0],x_n[1],label=str(target_names[y_n]),s=3)\n",
        "  plt.savefig('TSNE--{0}--{1:0.2f}.png'.format(name,100*(1-len(X)/len(self.data))),dpi=300,transparent=True,bbox_inches='tight')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sW4lfjm0j6B_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}